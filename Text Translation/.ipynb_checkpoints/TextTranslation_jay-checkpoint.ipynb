{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Translation \n",
    "\n",
    "Seq2seq encoder-decoder using Keras\n",
    "\n",
    "Jay Urbain, PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language translation task\n",
    "\n",
    "Translate from a language *X* to *English*.\n",
    "\n",
    "Below are 5 different results from different translation models. \n",
    "\n",
    "Even if we don't know what the original sentence was, the context of the original sentence is clear when following examples when considered as a group. \n",
    "\n",
    "Translation A: I ask him whether he will once again make a stand-up comedy tour.\n",
    "\n",
    "Translation B: I ask him if he will again make a stand-up comedy tour.\n",
    "\n",
    "Translation C: I wonder him if he will ever make a booth up comedy tour.\n",
    "\n",
    "Translation D: I ask him if he will ever make a stand-up comedy tour ever.\n",
    "\n",
    "Translation E: I ask him whether he will again make a stand-up comedy tour."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####TODO:\n",
    "\n",
    "Rank the translations from Best to Worst\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be relatively easy to spot the worst translation as it doesn't quite make sense in English when translated literally. That shows the difficulty of translating in general. Context has a significant impact on language translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/py3.6tf1.3keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Traceback (most recent call last):\n",
      "  File \"OpenSeq2Seq/run.py\", line 263, in <module>\n",
      "    main()\n",
      "  File \"OpenSeq2Seq/run.py\", line 58, in main\n",
      "    config_module = runpy.run_path(args.config_file, init_globals={'tf': tf})\n",
      "  File \"/Applications/anaconda/envs/py3.6tf1.3keras/lib/python3.6/runpy.py\", line 261, in run_path\n",
      "    code, fname = _get_code_from_file(run_name, path_name)\n",
      "  File \"/Applications/anaconda/envs/py3.6tf1.3keras/lib/python3.6/runpy.py\", line 231, in _get_code_from_file\n",
      "    with open(fname, \"rb\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'OpenSeq2Seq/example_configs/nmt.json'\n"
     ]
    }
   ],
   "source": [
    "!python OpenSeq2Seq/run.py --config_file=OpenSeq2Seq/example_configs/nmt.json --logdir=./noatt --mode=infer --inference_out=baseline.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let it run, you can open up [baseline.txt](baseline.txt) to see the translation as it's being written. Once you see \"I wonder him if he will ever make a booth up com@@ ed@@ y@@ tour .\" as the output then you can press Stop button or Interrupt Kernel. It should take about 3.5 minutes to run to reach this line. Last part of the lab will explain why the translations contain words like \"com@@\" due to byte pair encoding (BPE). You can simply eliminate those characters to get the sentences used above.\n",
    "\n",
    "So you can see this model produced the worst translation. On the other hand, identifying the best translation might differ from person to person since there's some subjectivity involved. Take Translation D for example, double use of 'ever' in one sentence probably lowers its score as a good English translation. Turns out, all the best results you have identified use **attention** to achieve those results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Explained\n",
    "If we want to understand something then paying attention is really helpful. In Neural Networks, it also helps to identify the most critical or important things to pay attention to. You can find resources on [attention](http://ruder.io/deep-learning-nlp-best-practices/index.html#attention) and current best practices for NLP in general. Mathematically we can also visualize attention with the following image:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/philipperemy/keras-attention-mechanism/blob/master/assets/attention_1.png?raw=true\" width=\"400\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the big spike is where the attention of the model will be directed. Our overall goal is to use attention in Machine Translation models but let's try to implement attention in a simpler problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Bidirectional\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get activations of attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_activations(model, inputs, layer_name=None):\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of simple random dataset for attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_recurrent(n, time_steps, input_dim, attention_column=None):\n",
    "    \"\"\"\n",
    "    Data generation. x is purely random except that it's first value equals the target y.\n",
    "    In practice, the network should learn that the target = x[attention_column].\n",
    "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
    "    :param n: the number of samples to retrieve.\n",
    "    :param time_steps: the number of time steps of your series.\n",
    "    :param input_dim: the number of dimensions of each element in the series.\n",
    "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
    "    :return: x: model inputs, y: model targets\n",
    "    \"\"\"\n",
    "    if attention_column is None:\n",
    "        attention_column = np.random.randint(low=0, high=input_dim)\n",
    "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 2\n",
    "TIME_STEPS = 20\n",
    "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "APPLY_ATTENTION_BEFORE_LSTM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Attention itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = Multiply(name='attention_mul')([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two places to put attention, in relation to the LSTM. This function will apply attention after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will apply attention before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_attention_applied_before_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    lstm_units = 32\n",
    "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can select where to apply the attention depending on whether APPLY_ATTENTION_BEFORE_LSTM is true or false. The following cell will generate data and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 300000\n",
    "inputs_1, outputs = get_data_recurrent(N, TIME_STEPS, INPUT_DIM)\n",
    "\n",
    "if APPLY_ATTENTION_BEFORE_LSTM:\n",
    "    m = model_attention_applied_before_lstm()\n",
    "else:\n",
    "    m = model_attention_applied_after_lstm()\n",
    "\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model and get the attention visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.fit([inputs_1], outputs, epochs=1, batch_size=512, validation_split=0.1)\n",
    "\n",
    "attention_vectors = []\n",
    "for i in range(300):\n",
    "    testing_inputs_1, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
    "    attention_vector = np.mean(get_activations(m,\n",
    "                                               testing_inputs_1,\n",
    "                                               layer_name='attention_vec')[0], axis=2).squeeze()\n",
    "    # print('attention =', attention_vector)\n",
    "    assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
    "    attention_vectors.append(attention_vector)\n",
    "\n",
    "attention_vector_final = np.mean(np.array(attention_vectors), axis=0)\n",
    "# plot part.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "pd.DataFrame(attention_vector_final, columns=['attention (%)']).plot(kind='bar',\n",
    "                                                                     title='Attention Mechanism as '\n",
    "                                                                           'a function of input'\n",
    "                                                                           ' dimensions.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Based on the graph above, where is the attention being applied?\n",
    "\n",
    "Input Dimension: fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Change the code so it's being applied after LSTM and rerun everything as required."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Where is the attention being applied now?\n",
    "\n",
    "Input Dimensions: fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Task - Translation\n",
    "\n",
    "Now that you understand how attention works and its effects on the model, let's turn back to our original task of translation. Machine translation is a well-known application and typical use for Natural Language Processing. Since 1950s, scientists have tried to create a model to automatically translate from say French to English. Nowadays, it became possible for machines to do the translation automatically and the attention mechanism has greatly increased the quality of the translation. Here the example image with attention map for the neural machine translation of sample phrase:\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.23.48-PM.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our lab we will concentrate on a much simpler task: we will translate dates from human readable to machine readable, eg. Oct 25th, 2017 to 2017-10-25. The inspiration comes from this [post](https://medium.com/datalogue/attention-in-keras-1892773a4f22). You can also follow it for more detailed background on this problem.\n",
    "\n",
    "To do this we need to understand one more concept - Sequence-to-Sequence language modeling.\n",
    "The idea of such architecture is here:\n",
    "<p aling=\"center\">\n",
    "<img src=\"https://talbaumel.github.io/attention/img/birnn.jpg\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "There is an Embedding layer at the bottom, the bidirectional RNN in the middle and softmax as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENCODER_UNITS = 32\n",
    "DECODER_UNITS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use more complex idea than simple seq2seq: we're adding two explicit parts of our network - encoder and decoder (on which attention is being applied). The explanatory picture for this idea is below:\n",
    "<p aling=\"center\"><img src=\"https://i.stack.imgur.com/Zwsmz.png\"></p>\n",
    "\n",
    "The lower part of the network is encoding the input to some hidden intermediate representation and the upper part is decoding the hidden representation into an actual readable output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets create a machine translation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_simple_nmt(in_chars, out_chars):\n",
    "    inputs = Input(shape=(TIME_STEPS,))\n",
    "    \n",
    "    input_embed = Embedding(in_chars, ENCODER_UNITS * 2, input_length=TIME_STEPS, trainable=True,\n",
    "                            name='embedding')(inputs)\n",
    "    \n",
    "    enc_out = Bidirectional(LSTM(ENCODER_UNITS, return_sequences=True))(input_embed)\n",
    "    dec_out = LSTM(DECODER_UNITS, return_sequences=True)(enc_out)\n",
    "    attention_mul = attention_3d_block(dec_out)\n",
    "    \n",
    "    output = TimeDistributed(Dense(out_chars, activation='softmax'))(attention_mul)\n",
    "   \n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate data. Our data will be dates in different text formats and in fixed output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "fake.seed(12345)\n",
    "random.seed(12345)\n",
    "\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "\n",
    "# change this if you want it to work with another language\n",
    "LOCALES = ['en_US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_date():\n",
    "    \"\"\"\n",
    "        Creates some fake dates \n",
    "        :returns: tuple containing human readable string, machine readable string, and date object\n",
    "    \"\"\"\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        human_readable = format_date(dt, format=random.choice(FORMATS), locale=random.choice(LOCALES))\n",
    "\n",
    "        case_change = random.choice([0,1,2])\n",
    "        if case_change == 1:\n",
    "            human_readable = human_readable.upper()\n",
    "        elif case_change == 2:\n",
    "            human_readable = human_readable.lower()\n",
    "        # if case_change == 0, do nothing\n",
    "\n",
    "        machine_readable = dt.isoformat()\n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "\n",
    "    return human_readable, machine_readable, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(n_examples):\n",
    "    \"\"\"\n",
    "        Creates a dataset with n_examples and vocabularies\n",
    "        :n_examples: the number of examples to generate\n",
    "    \"\"\"\n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "\n",
    "    for i in tqdm(range(n_examples)):\n",
    "        h, m, _ = create_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h, m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "\n",
    "    human = dict(zip(list(human_vocab) + ['<unk>', '<pad>'], \n",
    "                     list(range(len(human_vocab) + 2))))\n",
    "    inv_machine = dict(enumerate(list(machine_vocab) + ['<unk>', '<pad>']))\n",
    "    machine = {v:k for k,v in inv_machine.items()}\n",
    " \n",
    "    return dataset, human, machine, inv_machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_int(string, lenght, vocab):\n",
    "    if len(string) > lenght:\n",
    "        string = string[:lenght]\n",
    "        \n",
    "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "    \n",
    "    if len(string) < lenght:\n",
    "        rep += [vocab['<pad>']] * (lenght - len(string))\n",
    "    \n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int_to_string(ints, inv_vocab):\n",
    "    return [inv_vocab[i] for i in ints]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually generating data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 300000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = create_dataset(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling and training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = model_simple_nmt(len(human_vocab), len(machine_vocab))\n",
    "\n",
    "m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, targets = zip(*dataset)\n",
    "inputs = np.array([string_to_int(i, TIME_STEPS, human_vocab) for i in inputs])\n",
    "targets = [string_to_int(t, TIME_STEPS, machine_vocab) for t in targets]\n",
    "targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.fit([inputs], targets, epochs=1, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n",
    "\n",
    "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    prediction = np.argmax(prediction[0], axis=-1)\n",
    "    return int_to_string(prediction, inv_output_vocabulary)\n",
    "\n",
    "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n",
    "    predicted = []\n",
    "    for example in examples:\n",
    "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
    "        print('input:', example)\n",
    "        print('output:', predicted[-1])\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_examples(m, human_vocab, inv_machine_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the actual attention map on some example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_map(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    \"\"\"\n",
    "        visualization of attention map\n",
    "    \"\"\"\n",
    "    # encode the string\n",
    "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
    "\n",
    "    # get the output sequence\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    predicted_text = np.argmax(prediction[0], axis=-1)\n",
    "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
    "\n",
    "    text_ = list(text)\n",
    "    # get the lengths of the string\n",
    "    input_length = len(text)\n",
    "    output_length = predicted_text.index('<pad>')\n",
    "    # get the activation map\n",
    "    attention_vector = get_activations(model, [encoded], layer_name='attention_vec')[0].squeeze()\n",
    "    activation_map = attention_vector[0:output_length, 0:input_length]\n",
    "    \n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(8, 8.5))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "    # add image\n",
    "    i = ax.imshow(activation_map, interpolation='nearest', cmap='gray')\n",
    "\n",
    "    # add colorbar\n",
    "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Probability', labelpad=2)\n",
    "\n",
    "    # add labels\n",
    "    ax.set_yticks(range(output_length))\n",
    "    ax.set_yticklabels(predicted_text[:output_length])\n",
    "\n",
    "    ax.set_xticks(range(input_length))\n",
    "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "\n",
    "    # add grid and legend\n",
    "    ax.grid()\n",
    "\n",
    "    f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention_map(m, human_vocab, inv_machine_vocab, EXAMPLES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably see, the default model for this lab is not that good. But you could try to improve it by yourself. You could get better results, like this:\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/6295292/26899949-bbac0c7c-4b9e-11e7-84d6-c2f31166af07.png\" width=\"800\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Add layers and modify the code to improve results of the date translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before doing this part run Kernel->Restart so the GPU memory is completely free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the toy examples we finally see what attention is good for. Next, we will try an actual Neural Machine Translation model, which is shipped with this lab. This NMT model was trained on a German-English corpus, so it will translate from German to English.\n",
    "\n",
    "Before we start we need to discuss one more thing, which is really important in machine translation (and also other NLP tasks): BPE representation.\n",
    "\n",
    "### BPE\n",
    "BPE stands for byte pair encoding. It means that common byte pairs (bigrams of chars in our case) are replaced by a byte which never occurs in the corpus. Say, in our corpus we have never seen the \"#\" char, so we could use it to represent some typical bigram like \"ie\". But in practice all the printable chars are used, so for BPE the unprintable part of codepage is used. To actually print the text, we need to reformat it back. so you'll see in text \"@@ \" - these are artifacts from such renormalization.\n",
    "\n",
    "Here we have example text in German, which will be translated in English by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! head wmt/newstest2015.tok.bpe.32000.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual architecture used is:\n",
    "![](../2017-09-14_23-11-48.png)\n",
    "It is slightly more complex than in our previous task with dates. Here we again have encoder-decoder architecture, but the attention now is taken from all the input, not the part of it. Also we use the so called *context vector* which is representation of the whole sentence - it is helpful for the model to \"get the idea\" of a phrase before translating it.\n",
    "\n",
    "\n",
    "Lets finally see what our model will give us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python OpenSeq2Seq/run.py --config_file=OpenSeq2Seq/example_configs/nmt.json --logdir=./nmt --mode=infer --inference_out=pred.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up [pred.txt](pred.txt) to compare to [baseline.txt](baseline.txt) to see the difference attention makes in the overall quality of the translation. Training with more epochs will improve and possibly to get to the translation that you've identified as the best in the first part of the lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Acknowledgements__: code based on keras-visualize-activations of Philippe Remy\n",
    "\n",
    "URL: https://github.com/philipperemy/keras-visualize-activations\n",
    "\n",
    "The idea of date translation is borrowed from https://github.com/datalogue/keras-attention.\n",
    "\n",
    "For the real case we have used https://github.com/NVIDIA/OpenSeq2Seq, NVIDIA's implementation of Seq2Seq model.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
