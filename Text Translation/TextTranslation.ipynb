{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Translation \n",
    "\n",
    "Seq2seq LSTM encoder-decoder with attention\n",
    "\n",
    "Jay Urbain, PhD\n",
    "\n",
    "Acknowledgement: code based on keras-visualize-activations of Philippe Remy\n",
    "\n",
    "URL: https://github.com/philipperemy/keras-visualize-activations\n",
    "\n",
    "The idea of date translation is borrowed from https://github.com/datalogue/keras-attention.\n",
    "\n",
    "For the real case we have used https://github.com/NVIDIA/OpenSeq2Seq, NVIDIA's implementation of Seq2Seq model.\n",
    "\n",
    "Introductory resources for using `attention`:  \n",
    "http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Translation using seq2seq encoder-decoder with attention\n",
    "\n",
    "**Let's suppose we are tasked with translating from some language into English.** \n",
    "\n",
    "Below are 5 different results from different translation models. While we don't know what the original sentence might be, we can see that the context can be easily figured out from the following examples when considered as a group. \n",
    "\n",
    "Translation A: I ask him whether he will once again make a stand-up comedy tour.\n",
    "\n",
    "Translation B: I ask him if he will again make a stand-up comedy tour.\n",
    "\n",
    "Translation C: I wonder him if he will ever make a booth up comedy tour.\n",
    "\n",
    "Translation D: I ask him if he will ever make a stand-up comedy tour ever.\n",
    "\n",
    "Translation E: I ask him whether he will again make a stand-up comedy tour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "Rank the translations from Best to Worst\n",
    "\n",
    "Rank: Fill-in\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be relatively easy to spot poor translations since they don't quite make sense when translated into English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to skip the code below which uses the nVidia OpenSeq2Seq library to generate translations from [baseline.txt](baseline.txt). The models were too large to include within the Docker file. However, I ran it and included a few samples in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python OpenSeq2Seq/run.py --config_file=OpenSeq2Seq/example_configs/nmt.json --logdir=./noatt --mode=infer --inference_out=baseline.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">==================> Trying to restore from: ./noatt/model-59244\n",
    ">==================> Saving inference results to: baseline.txt\n",
    "Prime Minister India and Japan met in Tokyo . </S>\n",
    "Federal Government and Bun@@ de@@ stag not joined . </S>\n",
    "I wonder him if he will ever make a booth up com@@ ed@@ y@@ tour . </S>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running ~3.5 minutes (on a GPU) you'll see translations like \"I wonder him if he will ever make a booth up com@@ ed@@ y@@ tour .\" A good place to stop.\n",
    "\n",
    "The \"com@@\" is due to <a href='https://en.wikipedia.org/wiki/Byte_pair_encoding'>byte pair encoding (BPE)</a>. You can simply eliminate those characters for now.\n",
    "\n",
    "This shows the difficulty of translating in general. Notice how small phrases or word groups are easier to translate. E.g., \"stand-up comedy tour.\" But larger contextual depencies are problematic. We hope to solve this problem by using attention in our models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Bidirectional\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.backend import int_shape\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Translating human readable dates into machine readable dates\n",
    "\n",
    "Neural Machine Translation (NMT) is typically used to translate sentences from a source language (e.g. Arabic) to a target language (e.g. Hindi), however NMT has a wide range of applications. \n",
    "\n",
    "In this notebook, you're going to build an NMT model to translate human readable dates (e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\") into machine readable dates (e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"). We'll be standardizing the dates on the US date encoding of MM/dd/YYYY when generating the dates. The machine readable format is in YYYY-MM-dd format. \n",
    "\n",
    "You can review [nmt_utils.py](nmt_utils.py) to see the date formatting. Try to figure out how the formats work. This knowledge may help later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Dataset\n",
    "\n",
    "The dataset is a corpus of `1000` human readable dates and their equivalent machine readable dates. Run the following cell to load the dataset and print some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = create_dataset(m)\n",
    "\n",
    "Tx = 20\n",
    "# The single star * unpacks the sequence/collection into positional arguments\n",
    "sources, targets = zip(*dataset)\n",
    "sources = np.array([string_to_int(i, Tx, human_vocab) for i in sources])\n",
    "targets = [string_to_int(t, Tx, machine_vocab) for t in targets]\n",
    "targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've loaded:\n",
    "- dataset: a list of tuples (human readable date, machine readable date)\n",
    "- human_vocab: a python dictionary mapping all characters used in human readable dates to an index (integer values)\n",
    "- machine_vocab: a python dictionary mapping all characters used in machine readable dates to an index (integer values)\n",
    "- inv_machine_vocab: the inverse dictionary of `machine_vocab`\n",
    "- sources: a processed version of dataset's human readable dates, where each character is replaced by the integer value it is mapped to in the human_vocab, padded to $T_x$ values. `sources.shape = (m, Tx)`\n",
    "- targets: a processed version of dataset's machine readable dates, where each character is replaced by the one-hot vector of the character it is mapped to in the machine_vocab, the date is also padded up to $T_x$. `targets.shape = (m, Tx, len(machine_vocab))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the index in the cell below to navigate in the dataset, and see how source/target dates are preprocessed. The source uses an index, the target uses one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 12\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing:\", sources[index])\n",
    "print(\"Target after preprocessing:\", targets[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 - Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your NMT model will use an encoder-decoder architecture. One training example's run through the model is explained in the figures below.\n",
    "\n",
    "<img src=\"images/enc.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 1**: Simple NMT model (encoder part) </center></caption>\n",
    "\n",
    "First, a preprocessed source date is encoded using a Bi-directional LSTM. The sequence of hidden states are returned and stored in an object called `enc_out`. `enc_out` is then given as an input sequence to the decoder, which is described in the figure below.\n",
    "\n",
    "<img src=\"images/dec.png\" style=\"width:500;height:300px;\"> <br>\n",
    "<caption><center> **Figure 2**: Simple NMT model (decoder part) </center></caption>\n",
    "\n",
    "The output vectors can then be compared to the ground truth using a categorical cross-entropy loss function. Minimizing this loss function will give you a trained NMT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:   \n",
    "Review the following code.\n",
    "\n",
    "Implement the `model_simple_nmt()` described in the figure above. The LSTMs both use 32 units. The embedding layer outputs a 64-dimensional embedding and should be trainable. These functions might be useful: Input(), [Embedding()](https://keras.io/layers/embeddings/), [LSTM()](https://keras.io/layers/recurrent/#lstm), [Bidirectional()](), [Dense()](https://keras.io/layers/core/#dense), [TimeDistributed()](https://keras.io/layers/wrappers/#timedistributed), [Model()](https://keras.io/models/model/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_simple_nmt(human_vocab_size, machine_vocab_size, Tx = 20):\n",
    "    \"\"\"\n",
    "    Simple Neural Machine Translation model\n",
    "    \n",
    "    Arguments:\n",
    "    human_vocab_size -- size of the human vocabulary for dates, it will give us the size of the embedding layer\n",
    "    machine_vocab_size -- size of the machine vocabulary for dates, it will give us the size of the output vector\n",
    "    \n",
    "    Returns:\n",
    "    model -- model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Define the input of your model with a shape (Tx,)\n",
    "    inputs = Input(shape=(Tx,))\n",
    "    \n",
    "    # Define the embedding layer. This layer should be trainable and the input_length should be set to Tx.\n",
    "    input_embed = Embedding(human_vocab_size, 2*32, input_length = Tx, trainable=True)(inputs)\n",
    "    \n",
    "    # Encode the embeddings using a bidirectional LSTM\n",
    "    enc_out = Bidirectional(LSTM(32, return_sequences=True))(input_embed)\n",
    "    \n",
    "    # Decode the encoding using an LSTM layer\n",
    "    dec_out = LSTM(32, return_sequences=True)(enc_out)\n",
    "    \n",
    "    # Apply Dense layer to every time steps\n",
    "    output = TimeDistributed(Dense(machine_vocab_size, activation='softmax'))(dec_out)\n",
    "    \n",
    "    # Create model instance \n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create your `model` and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = model_simple_nmt(len(human_vocab), len(machine_vocab), Tx)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's train this model for 200 epochs by changing and running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "model.fit([sources], targets, epochs=10, batch_size=512, validation_split=0.1)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.4 - Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your simple NMT model on various examples using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where you adjust examples\n",
    "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '3rd of March 2001']\n",
    "\n",
    "def run_examples(examples):\n",
    "    for example in examples:\n",
    "        source = string_to_int(example, Tx, human_vocab)\n",
    "        prediction = model.predict(np.array([source]))\n",
    "        prediction = np.argmax(prediction[0], axis = -1)\n",
    "        output = int_to_string(prediction, inv_machine_vocab)\n",
    "        print(\"source:\", example)\n",
    "        print(\"output:\", ''.join(output))\n",
    "    \n",
    "run_examples(EXAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: 3 May 1979 <br>\n",
    "output: 1979-05-03\n",
    "\n",
    "source: 5 Apr 09 <br>\n",
    "output: 2009-04-05\n",
    "\n",
    "source: 20th February 2016 <br>\n",
    "output: 2016-09-20\n",
    "\n",
    "source: Wed 10 Jul 2007 <br>\n",
    "output: 2007-04-10\n",
    "\n",
    "source: Saturday May 9 2018 <br>\n",
    "output: 2018-05-28\n",
    "\n",
    "source: March 3 2001 <br>\n",
    "output: 2001-03-03\n",
    "\n",
    "source: March 3rd 2001 <br>\n",
    "output: 2000-03-03\n",
    "\n",
    "source: 3rd of March 2001 <br>\n",
    "output: 2000-12-03\n",
    "\n",
    "It does relatively well as the first two examples show.\n",
    "\n",
    "We can intuit that the longer the string, and the more items to retain while doing the translation, the bigger the errors tend to be. Even for us, when we want to improve our results, we have to pay *attention* to the task we are currently doing. So intuitively, if we provide Neural Networks the ability to give more attention to relevant sections and also retain information over the long inputs, then results should improve. To that end, you are going to add an attention mechanism to your NMT and see if it improves the results or not.\n",
    "\n",
    "#### TODO:\n",
    "\n",
    "Now it's your turn to create your test set and produce similar output. Remember all the data formats that we asked you to investigate? Now create 5 formats that do not appear in EXAMPLES (code cell above) that we have given for your test set below. Then complete the code so prediction is run on all of them and you output a similar output string as we have shown:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Improving results using attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. You would first read the paragraph. Then, while translating, you would read and focus on the parts of the paragraph corresponding to the parts you are currently translating.\n",
    "\n",
    "Attention mechanism is a technique in Deep Learning to help the model drive its focus onto important parts of the input. In this part, you will augment your simple NMT with attention. \n",
    "\n",
    "#### 2.1 - Attention models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/NMT_Components.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 3**: Attention NMT model</center></caption>\n",
    "\n",
    "Concretely, after running a source date in the encoder (Bi-LSTM), you would like to give all the hidden states to the decoder to get back the target date. However, at every step in the decoding process, you'd like your model to be able to tell which hidden state is more important to use. So, inserting the attention block between encoder and decoder is the new architecture we will use to decipher the important parts of the input to pay attention to. \n",
    "\n",
    "<img src=\"images/Attention_mechanism.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 4**: Attention Implementation</center></caption>\n",
    "\n",
    "The above figure contains enlarged details of the attention block itself. The attention block takes the outputs from the encoder, and applies different weights to them with tanh as the activation function. Softmax will choose the winning vector and apply attention to that output from the encoder. There are various ways and methods of implementing an attention mechanism, but the basic principle is the same. It will choose the most relevant part of the input to focus the attention and retain knowledge over long inputs in order to provide a better translation.\n",
    "\n",
    "Now, let's implement a simplified attention block using softmax and dimension reduction on vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    \"\"\"\n",
    "    Implement the attention block applied between two layers\n",
    "    \n",
    "    Argument:\n",
    "    inputs -- output of the previous layer, set of hidden states\n",
    "    \n",
    "    Returns:\n",
    "    output_attention_mul -- inputs weighted with attention probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_h and Tx from inputs' shape. Recall: inputs.shape = (m, Tx, n_h)\n",
    "    Tx = int_shape(inputs)[1]\n",
    "    n_h = int_shape(inputs)[2]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Permute inputs' columns to compute \"a\" of shape (m, n_h, Tx)\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    \n",
    "    # Apply a Dense layer with softmax activation. It should contain Tx neurons. a.shape should still be (m, n_h, Tx).\n",
    "    a = Dense(Tx, activation='softmax')(a)\n",
    "    \n",
    "    # Compute the mean of \"a\" over axis=1 (the \"hidden\" axis: n_h). a.shape should now be (m, Tx)\n",
    "    a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "    \n",
    "    # Repeat the vector \"a\" n_h times. \"a\" should now be of shape (m, n_h, Tx)\n",
    "    a = RepeatVector(n_h)(a)\n",
    "    \n",
    "    # Permute the 2nd and the first column of a to get a probability vector of attention. a_probs.shape = (m, Tx, n_h)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    # Apply the attention probabilities to the \"inputs\" by multiplying element-wise.\n",
    "    output_attention_mul = Multiply(name='attention_mul')([inputs, a_probs])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Assessing the efficiency of this attention block\n",
    "\n",
    "In order to ensure that the activation block you have just coded is able to tell the model where to focus, you are going to try an experiment.\n",
    "\n",
    "Let's first generate datapoints $(X, Y) = $ {$(x^{(i)}, y^{(i)})_{i=1...m}$} such that:\n",
    "- $y^{(i)}$ is a label equal to 0 or 1.\n",
    "- $x^{(i)}$ is a matrix of shape $(T_x, n_h)$ where one column is equal to $y^{(i)}$  and the rest is random. It means that for some time-step \"t\", we have x[t,:] = y.\n",
    "- Thus `X.shape =` $(m, T_x, n_h)$ and `Y.shape =` $(m, 1)$.\n",
    "\n",
    "Lets generate some fake data with actual answers in one of the dimensions. If our attention model is working, it will point out the column with answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_recurrent(n, time_steps, input_dim, attention_column=None):\n",
    "    \"\"\"\n",
    "    Data generation. x is purely random except that it's first value equals the target y.\n",
    "    In practice, the network should learn that the target = x[attention_column].\n",
    "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
    "    :param n: the number of samples to retrieve.\n",
    "    :param time_steps: the number of time steps of your series.\n",
    "    :param input_dim: the number of dimensions of each element in the series.\n",
    "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
    "    :return: x: model inputs, y: model targets\n",
    "    \"\"\"\n",
    "    if attention_column is None:\n",
    "        attention_column = np.random.randint(low=0, high=input_dim)\n",
    "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see some examples of X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x, y = get_data_recurrent(n = 2, time_steps = 4, input_dim = 3, attention_column=None)\n",
    "print(\"x.shape =\", x.shape)\n",
    "print(\"x =\", x)\n",
    "print()\n",
    "print(\"y.shape =\", y.shape)\n",
    "print(\"y =\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset (X, Y) is useful to observe the impact of attention. Indeed, you will try a binary classifier that given x will predict y. Thanks to the attention mechanism, your network should understand that only one time-step of x is useful to predict y, the rest is random. Your model should focus solely on this particular time-step. Run the following cell to load the dataset with attention on the third time-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_data_recurrent(n = 10000, time_steps = 20, input_dim = 2, attention_column = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 - Attention along with LSTM\n",
    "\n",
    "The attention block can be mounted at different positions in the network. The most common positions are before and after an LSTM. Below, we are providing two different models with the attention block applied before and after the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_applied_before_lstm(Tx, n_h):\n",
    "    \"\"\"\n",
    "    Model with attention applied BEFORE the LSTM\n",
    "        \n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model with a shape (Tx,)\n",
    "    inputs = Input(shape=(Tx, n_h,))\n",
    "    # Add the attention block\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    # Pass the inputs in a LSTM layer, return the sequence of hidden states\n",
    "    attention_mul = LSTM(32, return_sequences=False)(attention_mul)\n",
    "    # Apply Dense layer with sigmoid activation the output should be a single number.\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    # Create model instance \n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_applied_after_lstm(Tx, n_x):\n",
    "    \"\"\"\n",
    "    Model with attention applied AFTER the LSTM\n",
    "    \n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model with a shape (Tx,)\n",
    "    inputs = Input(shape=(Tx, n_x,))\n",
    "    # Pass the inputs in a LSTM layer, return the sequence of hidden states\n",
    "    lstm_out = LSTM(32, return_sequences=True)(inputs)\n",
    "    # Add the attention block\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    # Flatten the output of the attention block\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    # Apply Dense layer with sigmoid activation the output should be a single number.\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    # Create model instance \n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will train the model with attention applied after the LSTM. Later on, you can come back and try the model with attention applied before the LSTM. One advantage of applying attention before the LSTM on the inputs directly is that, it is easier to reason about. The probability distribution spans across the input dimensions only. Whereas, applying to the output of the LSTM makes the dimensional space much more complex to interpret and reason about but more commonly used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model_attention_applied_after_lstm(Tx = 20, n_x = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to compile the model you've just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "Running the cell below will train your model over the (X, Y) dataset for 100 epochs and a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "m.fit([X], Y, epochs=10, batch_size=16, validation_split=0.1)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define get_activations so we can extract out relevant information from the layers to analyze our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, inputs, layer_name=None):\n",
    "    \"\"\"\n",
    "    For a given Model and inputs, find all the activations in specified layer\n",
    "    If no layer then use all layers\n",
    "    \n",
    "    Returns:\n",
    "    activations from all the layer(s)\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "Define all the variables and parameters below. This is where you'll modify parameters later in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Set the input dimensions\n",
    "INPUT_DIM = 2\n",
    "\n",
    "# Set time steps\n",
    "TIME_STEPS = 20\n",
    "\n",
    "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "# Set whether the attention vector is shared\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "\n",
    "# Set the attention model in relation to LSTM\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False\n",
    "\n",
    "# Set the size of the dataset\n",
    "N = 300000\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have determined the parameters you want to experiment with. Run the following cell and let the model compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1, outputs = get_data_recurrent(N, TIME_STEPS, INPUT_DIM, attention_column=3)\n",
    "\n",
    "if APPLY_ATTENTION_BEFORE_LSTM:\n",
    "    m = model_attention_applied_after_lstm(Tx = 20, n_x = 2)\n",
    "else:\n",
    "    m = model_attention_applied_before_lstm(Tx = 20, n_h = 2)\n",
    "\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if your model and parameters worked, let's generate 300 training examples with `attention_column=3` by calling get_activations on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit([inputs_1], outputs, epochs=1, batch_size=512, validation_split=0.1)\n",
    "\n",
    "attention_vectors = []\n",
    "for i in range(300):\n",
    "    # Generate one training example (x, y), the attention column can be on any time-step.\n",
    "    testing_inputs_1, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM, attention_column=3)\n",
    "    # Extract the attention vector predicted by the model \"m\" on the training example \"x\".\n",
    "    attention_vector = np.mean(get_activations(m,\n",
    "                                               testing_inputs_1,\n",
    "                                               layer_name='attention_vec')[0], axis=2).squeeze()\n",
    "    # Append the attention vector to the list of attention vectors\n",
    "    assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
    "    attention_vectors.append(attention_vector)\n",
    "# Compute the average attention on every time-step\n",
    "\n",
    "attention_vector_final = np.mean(np.array(attention_vectors), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the plot to show where the attention is being applied and its effect on the results of the model by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot part.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "pd.DataFrame(attention_vector_final, columns=['attention (%)']).plot(kind='bar',\n",
    "                                                                     title='Attention Mechanism as '\n",
    "                                                                           'a function of input'\n",
    "                                                                           ' dimensions.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: \n",
    "\n",
    "Repeat all the steps using attention block before LSTM. Note all the variable and code changes required to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.3 - Adding Attention to your NTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention in the previous toy task was a nice extra. However, in this new date translation task, you cannot make your model work properly without it.\n",
    "\n",
    "Now it's time to add attention to the NMT model you've implemented in part (1). \n",
    "\n",
    "#### TODO: \n",
    "\n",
    "Re-implement `model_simple_nmt()` but this time with attention as `model_attention_nmt()`. Try to achieve better results by adding units, layers, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_nmt(human_vocab_size, machine_vocab_size, Tx = 20):\n",
    "    \"\"\"\n",
    "    Attention Neural Machine Translation model\n",
    "    \n",
    "    Arguments:\n",
    "    human_vocab_size -- size of the human vocabulary for dates, it will give us the size of the embedding layer\n",
    "    machine_vocab_size -- size of the machine vocabulary for dates, it will give us the size of the output vector\n",
    "    \n",
    "    Returns:\n",
    "    model -- model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Define the input of your model with a shape (Tx,)\n",
    "    inputs = Input(shape=(Tx,))\n",
    "    \n",
    "    # Define the embedding layer. This layer should be trainable and the input_length should be set to Tx.\n",
    "    input_embed = Embedding(human_vocab_size, 2*32, input_length = Tx, trainable=True)(inputs)\n",
    "    \n",
    "    # Encode the embeddings using a bidirectional LSTM\n",
    "    enc_out = Bidirectional(LSTM(32, return_sequences=True))(input_embed)\n",
    "    \n",
    "    # Add attention\n",
    "    attention = attention_3d_block(enc_out)\n",
    "    \n",
    "    # Decode the encoding using an LSTM layer\n",
    "    dec_out = LSTM(32, return_sequences=True)(attention)\n",
    "    \n",
    "    # Apply Dense layer to every time steps\n",
    "    output = TimeDistributed(Dense(machine_vocab_size, activation='softmax'))(dec_out)\n",
    "    \n",
    "    # Create model instance \n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/enc_attention.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 5**: Attention Model</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model_att = model_attention_nmt(len(human_vocab), len(machine_vocab))\n",
    "\n",
    "# Compile model\n",
    "model_att.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/train_infer.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 6**: Training vs Inferencing</center></caption>\n",
    "\n",
    "Now train the model by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_att.fit([sources], targets, epochs=1, batch_size=12, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (â‰ˆ1 line)\n",
    "example = \"3rd of March 2001\"\n",
    "### END CODE HERE ###\n",
    "source = string_to_int(example, 20, human_vocab)\n",
    "prediction = model_att.predict(np.array([source]))\n",
    "prediction = np.argmax(prediction[0], axis = -1)\n",
    "output = int_to_string(prediction, inv_machine_vocab)\n",
    "print(\"source:\", example)\n",
    "print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention\n",
    "\n",
    "If you were to run the poorly trained model in this assignment, then you might get an attention map like the following:\n",
    "<img src=\"images/poorly_trained_model.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 7**: Simple Attention Map</center></caption>\n",
    "\n",
    "Translation of 3 May 1979 will essentially be nonsense, with a bunch of 0s and 1's without translating to a valid date string. The attention is being applied to only one character y, and hence the translation does not work.\n",
    "\n",
    "<img src=\"images/date_attention.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 8**: Full Attention Map</center></caption>\n",
    "\n",
    "A well trained model can handle a tougher translation task, such as Saturday 9 May 2018. Here you can see that the attention completely ignores Saturday, which is not helpful to our task. 9 has been translated as 09 and May has been correctly translated as 05 by giving more attention to M. The year mostly requires it to differentiate on 18 to gather 2018 as the result.\n",
    "\n",
    "Attention map will generate the similar results for your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def attention_map(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    \"\"\"\n",
    "        visualization of attention map\n",
    "    \"\"\"\n",
    "    # encode the string\n",
    "    encoded = string_to_int(text, 20, input_vocabulary)\n",
    "\n",
    "    # get the output sequence\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    predicted_text = np.argmax(prediction[0], axis=-1)\n",
    "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
    "\n",
    "    text_ = list(text)\n",
    "    # get the lengths of the string\n",
    "    input_length = len(text)\n",
    "    output_length = predicted_text.index('<pad>') if '<pad>' in predicted_text else len(predicted_text)\n",
    "    # get the activation map\n",
    "    attention_vector = get_activations(model, [encoded], layer_name='attention_vec')[0].squeeze()\n",
    "    activation_map = attention_vector[0:output_length, 0:input_length]\n",
    "    \n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(8, 8.5))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "    # add image\n",
    "    i = ax.imshow(activation_map, interpolation='nearest', cmap='gray')\n",
    "\n",
    "    # add colorbar\n",
    "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Probability', labelpad=2)\n",
    "\n",
    "    # add labels\n",
    "    ax.set_yticks(range(output_length))\n",
    "    ax.set_yticklabels(predicted_text[:output_length])\n",
    "\n",
    "    ax.set_xticks(range(input_length))\n",
    "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "\n",
    "    # add grid and legend\n",
    "    ax.grid()\n",
    "\n",
    "#     f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run attention_map to generate the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_map(model_att, human_vocab, inv_machine_vocab, EXAMPLES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - BLEU score\n",
    "\n",
    "In this final part, you are going to implement the Bilingual Evaluation Understudy (BLEU) Score to assess the effectiveness of translations as a ratio. Completely mismatched results will be scored as 0.0, and the perfect translation will be scored as 1.0. There are various formulas applied to sentence, corpus or any length translation, but it's meant to be a good proxy to compare various implementations. Our date translation examples are simple, but BLEU score is also meant to encapuslate how a human would rate the translation for language translations. \n",
    "\n",
    "<img src=\"images/BLEU.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 9**: BLEU Score</center></caption>\n",
    "\n",
    "The BLEU could be described as the overlap of single tokens and sequences of tokens (2, 3 or 4) - this is $precision_i$. \n",
    "\n",
    "And also brevity penalty of $\\frac{output-length}{reference - length}$ is applied in the formula.\n",
    "\n",
    "We will split each ground truth to one character at a time and also the predicted value. BLEU score works on the 1-4 ngrams so we need to supply at least 4 ngrams in our predictions. If we had a string 2007-07-11, we can split it into tokens 2007, 07 and 11. But that makes BLEU scoring more difficult. So we are instead going to convert it to list of ['2', '0', '0', '7', '-', '0', '7', '-', '1', '1'] by making a token of every single character, including dashes.\n",
    "\n",
    "We will be using NLTK package's implementation of bleu_score to calculate. Generate BLEU score for your simple nmt by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '3rd of March 2001']\n",
    "GROUND_TRUTH = ['1979-05-03', '2009-04-05', '2016-02-20', '2007-07-11', '2018-05-09', '2001-03-03', '2001-03-03', '2001-03-03']\n",
    "\n",
    "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    encoded = string_to_int(text, 20, input_vocabulary)\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    prediction = np.argmax(prediction[0], axis=-1)\n",
    "    return int_to_string(prediction, inv_output_vocabulary)\n",
    "\n",
    "def run_examples(model, input_vocabulary, inv_output_vocabulary, samples=(EXAMPLES, GROUND_TRUTH)):\n",
    "    predicted = []\n",
    "    examples, targets = samples\n",
    "    assert len(examples) == len(targets)\n",
    "    \n",
    "    for i, example in enumerate(examples):\n",
    "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
    "        print('input:', example)\n",
    "        chstr = targets[i] # .split('-')\n",
    "        print('reference:', chstr)\n",
    "        pdstr = predicted[i]\n",
    "        ### START CODE HERE ###\n",
    "        pdstr = pdstr.replace('<pad>','')\n",
    "        ### END CODE HERE ###\n",
    "        #pdstr = pdstr.split('-\n",
    "        print('output:', pdstr)\n",
    "        cc = SmoothingFunction()\n",
    "        print(\"BLEU score: \", sentence_bleu([[ch for ch in chstr]], [pd for pd in pdstr], smoothing_function=cc.method4))\n",
    "        print()\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_examples(model, human_vocab, inv_machine_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Generate the right BLEU scores\n",
    "\n",
    "The translation is correct for but how come BLEU score is low? It should be 1.0 yet it's XXX Hint: Print out the actual predicted value and check the tokenized list to see what needs to be done\n",
    "\n",
    "**TODO**: Generate your model_attention_nmt()'s BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Improve your `model_attention_nmt()`'s BLEU score as close to 1.0 as you can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE\n",
    "BPE stands for byte pair encoding. It means that common byte pairs (bigrams of chars in our case) are replaced by a byte which never occurs in the corpus. Say, in our corpus we have never seen the \"#\" char, so we could use it to represent some typical bigram like \"ie\". But in practice all the printable chars are used, so for BPE the unprintable part of codepage is used. To actually print the text, we need to reformat it back. So you'll see in text \"@@ \" - these are artifacts from such renormalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Optional TODO - Experiment with Beam Search\n",
    "\n",
    "As you have discovered, attention spreads the probability of which character to pay attention to in a given sequence. There are other strategies we can apply to improve the results of the translation. Beam search is one such heuristic that is popular. Beam search works by returning a list of output sequences that are most likely for a given input. It does this by exploring all of the possible next steps, and then chooses the most promising candidates from all possibilities. Those candidates form a 'beam' of directions, where the algorithm expands the search through each candidate's sequence of probabilities. \n",
    "\n",
    "The larger the beam, the better the performance of the model. Since there are more candidate sequences, this means the likelihood of finding the right answer increases. However, it requires more decoding, meaning a direct tradeoff with the quality of results versus accuracy. Seeding beam search with the most likely next words in a sequence can yield good results. The beam search would generate the resulting sequence, starting from the first word and appending, while exploring all the candidates (size of the beam) every step of the way.\n",
    "\n",
    "We have started the beginning of beam search to generate samples with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model_attention_nmt(len(human_vocab), len(machine_vocab))\n",
    "\n",
    "m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = zip(*dataset)\n",
    "inputs = np.array([string_to_int(i, 20, human_vocab) for i in inputs])\n",
    "targets = [string_to_int(t, 20, machine_vocab) for t in targets]\n",
    "targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit([inputs], targets, epochs=1, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_rnn_predict(samples, empty=human_vocab[\"<pad>\"], rnn_model=m, maxlen=30):\n",
    "    \"\"\"for every sample, calculate probability for every possible label\n",
    "    you need to supply your RNN model and maxlen - the length of sequences it can handle\n",
    "    \"\"\"\n",
    "    data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty)\n",
    "    return rnn_model.predict(data, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll try to generate some dates from our model to demostrate beam search properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beamsearch(predict=keras_rnn_predict, k=1, maxsample=10, \n",
    "               use_unk=False, \n",
    "               oov=human_vocab[\"<unk>\"], \n",
    "               empty=human_vocab[\"<pad>\"], \n",
    "               eos=human_vocab[\"<unk>\"]):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    \n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [[empty]]\n",
    "    live_scores = [0]\n",
    "\n",
    "    while live_k and dead_k < k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        probs = predict(live_samples, empty=empty)\n",
    "\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        if not use_unk and oov is not None:\n",
    "            cand_scores[:,oov] = 1e20\n",
    "        cand_flat = cand_scores.flatten()\n",
    "\n",
    "        # find the best (lowest) scores we have from all possible samples and new words\n",
    "        ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
    "        live_scores = cand_flat[ranks_flat]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = probs.shape[1]\n",
    "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_flat]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]  # remove first label == empty\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    return live_samples + dead_samples, live_scores + dead_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goldstar TODO**: \n",
    "\n",
    "Implement beam search and improve BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3.6tf1.3keras]",
   "language": "python",
   "name": "conda-env-py3.6tf1.3keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
