{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Sentiment Classification\n",
    "\n",
    "Jay Urbain, PhD\n",
    "\n",
    "#### Predict Sentiment From Movie Reviews Using Deep Learning\n",
    "\n",
    "We will be training a neural network for text classification. In text classification, documents or segments of text are assigned to different categories. For example, categories could be positive or negative sentiment, different authors, different topics, or different writing styles.\n",
    "\n",
    "In this lab, you will discover how you can predict the sentiment of movie reviews as either positive or negative in Python using the Keras deep learning library.\n",
    "\n",
    "We will build several models and evaluate the results.\n",
    "\n",
    "Credits:  \n",
    "TensorFlow, https://www.tensorflow.org/tutorials/.  \n",
    "Froncois Challet, 'Deep Learning with Python.'\n",
    "\n",
    "Topic:  \n",
    "- NLP terminology  \n",
    "- IMDB dataset description\n",
    "- Dataset preparation  \n",
    "- Language embeddings  \n",
    "- Develop and evaluate a multi-layer perceptron (MLP)  \n",
    "- Develop and evaluate a one-dimensional CNN  \n",
    "- Develop and evaluate a one-dimensional RNN  \n",
    "- Model comparison and analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP terminology for describing input data:\n",
    "\n",
    "- `token` - A unit of text, typically a work, but it could also be a phrase like \"New York\", a sub-word like \"mega\" in \"megabyte\", or a letter like \"m\". Each token can be represented as a distinct number in a process called *tokenizing*. \n",
    "- `document` - A sequence of *tokens.* This could be whole book or a tweet. In this case, we're going to classify each *movie review* as having a positive or a negative sentiment.  \n",
    "- `corpus` - A set of *documents.* You can think of this as your \"dataset\". If you want to learn about language in general, you might use a *corpus* like Wikipedia. In this case, we'll use a *corpus* containing movie reviews since each *document* is paired with a rating indicating *sentiment.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term-document metrics\n",
    "\n",
    "Terms in matrices can be represented by their presence in a document (1/0), term-frequency, `TF-IDF`, a widely used measure of word *importance*, or other application specific metric. In our application, we will use overall word sentiment.\n",
    "\n",
    "`TF` (*Term Frequency*), is the number of times a particular term $w$ appears in a document $d$.  \n",
    "\n",
    "$$TF(w, d) = \\frac{count(w, d)}{\\sum_{v \\in V}count(v, d)}$$  \n",
    "where $w, v$ words, in a document $d$, over the corpus vocabulary $V$.\n",
    "\n",
    "`DF` (*Document Frequency*) is the number of documents a particular word $w$ occurs.\n",
    "\n",
    "`IDF` (*Inversed Document Frequency*) represents how distinctive a word is: \n",
    "\n",
    "$$IDF(w) = log \\frac{|D|}{\\sum_{d \\in D}\\mathbb{1}(w, d)} $$\n",
    "where $D$ is a corpus of documents, $\\mathbb{1}$.\n",
    "\n",
    "`TF-IDF` is the product of $TF*IDF$ and assigns weights to words based on their frequency within a document ($TF$), and their overall distinctiveness ($IDF$) within the corpus.\n",
    "\n",
    "`Sentiment value` for our task will be using the overall sentiment for each individual word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "The ```Tokenizer``` class in Keras allows you to vectorize a text corpus, by turning each text/document into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf, etc.\n",
    "\n",
    "Keras provides the `Tokenizer` class for preparing text documents for deep learning. The Tokenizer must be constructed, and then fit on either raw text documents or integer encoded text documents.\n",
    "\n",
    "Once the Tokenizer has been fit on training data, it can be used to encode documents in the train or test datasets.\n",
    "\n",
    "The `texts_to_matrix()` function on the `Tokenizer` can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary.\n",
    "\n",
    "This function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function.\n",
    "\n",
    "Once fit, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:  \n",
    "- word_counts: A dictionary of words and their counts.  \n",
    "- word_docs: A dictionary of words and how many documents each appeared in.  \n",
    "- word_index: A dictionary of words and their uniquely assigned integers.  \n",
    "- document_count:An integer count of the total number of documents that were used to fit the Tokenizer.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a basic of example of using the Tokenizer class where we encode the sample text using binary, count, and tfidf encoding respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Great acting',\n",
    "\t\t'Waste of time to see this movie',\n",
    "\t\t'Great acting effort',\n",
    "\t\t'Major looser',\n",
    "\t\t'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "# integer encode documents\n",
    "print(\"Binary\")\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='binary')\n",
    "print(encoded_docs)\n",
    "\n",
    "print(\"\\nCount\")\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)\n",
    "\n",
    "print(\"\\ntfidf\")\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='tfidf')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB dataset\n",
    "\n",
    "The dataset we will be using is the `Large Movie Review Dataset` often referred to as the IMDB dataset.\n",
    "\n",
    "The IMDB dataset was constructed from a collection of 50,000 reviews from IMDB, allowing no more than 30 reviews per movie. The constructed dataset contains an even number of positive and negative reviews, so randomly\n",
    "guessing yields 50% accuracy. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10.\n",
    "\n",
    "The data was collected by Stanford researchers and was used in the 2011 paper: <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">Learning Word Vectors for Sentiment Analysis</a>, where an accuracy of 88.89% was achieved.\n",
    "\n",
    "Challenges with classifying sentiment can be shown in the following example which contains positive sentiment words like *funny* and *super witty*, but the overal sentiment of the review is clearly negative. \n",
    "\n",
    "<blockquote>\n",
    "\"*This movie was actually neither that funny, nor super witty.*\"\n",
    "</blockquote>\n",
    "\n",
    "To identify the correct sentiment, we will need to develop models that capture word context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preparation\n",
    "\n",
    "The `keras.datasets.imdb.load_data()` function allows you to load the dataset in a format that is ready for use in neural network and deep learning models. So we'll be able to skip the data cleansing and tokenization step discussed above. *It's important to understand that in most practical applications, this is a significant amount of work.*\n",
    "\n",
    "Each review in the dataset is labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integer encoded). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "Calling `imdb.load_data()` the first time will download the IMDB dataset to your computer and store it in your home directory under ~/.keras/datasets/imdb.pkl as a 32 megabyte file.\n",
    "\n",
    "`imdb.load_data()` provides additional arguments including the number of top words to load (where words with a lower integer are marked as zero in the returned data), the number of top words to skip (to avoid the “the”‘s) and the maximum length of reviews to support.\n",
    "\n",
    "Let’s load the dataset and calculate some properties of it. We will start off by loading some libraries and loading the entire IMDB dataset as a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot\n",
    "# load the dataset\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first review. \n",
    "\n",
    "*Note: integers that indicate the absolute popularity of the word in the dataset.* This technique was developed in the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the IMDB word index and display the text for the first review.\n",
    "\n",
    "*Note: the first 3 indices are reserved for padding, start, and unknown word*\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "NUM_WORDS=1000 # only use top 1000 words\n",
    "INDEX_FROM=3   # word index offset\n",
    "\n",
    "# train,test = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "# train_x,train_y = train\n",
    "# test_x,test_y = test\n",
    "\n",
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in X[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the sentiment label of the first ten reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "Display a few other reviews along with their sentiment in the cell(s) below. You can use the following print statment and adjust the index:\n",
    "\n",
    "```\n",
    "    print(' '.join(id_to_word[id] for id in X[0] ))\n",
    "```\n",
    "\n",
    "Record your observations about postive and negative sentiment reviews. What makes reviews positive or negative? Can you find any tricky examples where the true sentiment may be difficult to judge? Please share your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can display the shape of the training dataset. We should have 25,000 training and 25,000 testing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Test data: \")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the unique class values (encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can get an idea of the total number of unique words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can get an idea of the average review length.\n",
    "\n",
    "Looking at a box and whisker plot for the review lengths in words, we can probably cover most of the mass of the word distribution with a clipped length of 400 to 500 words.\n",
    "\n",
    "In a box plot, the rectangle includes the center 50 percentile (inter quartile range) of the distribution. The red horizontal bar in the box is the median. The whisker (horizonal bars) are by default +/-1.5 the IQR. Dots drawn outside of the whiskers, are considered outliers.\n",
    "\n",
    "https://matplotlib.org/api/_as_gen/matplotlib.pyplot.boxplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks don’t take raw text as input. Only numeric tensors.\n",
    "\n",
    "Vectorizing text is the process of transforming text into numeric tensors. This can be done in multiple ways:   \n",
    "- Segment text into words, and transform each word into a vector.   \n",
    "- Segment text into characters, and transform each character into a vector.   \n",
    "- Extract n-grams of words or characters, and transform each n-gram into a vector.   \n",
    "- Apply categorical transformation: One-hot encoding or word embedding.\n",
    "\n",
    "\n",
    "*One-hot*   \n",
    "Words are categorical, they have no ordinal relationship. So they can’t just be numerically encoded.\n",
    "For each word:  \n",
    "- Numerically encode each word: {red:0, green:1, blue:2}  \n",
    "- Generate vector of zeros the length of all possible words/categories  \n",
    "- Set the index value for the word in the vector  \n",
    "\n",
    "|red|green|blue|\n",
    "|-----|-----|-----|\n",
    "|1|0|0|\n",
    "|0|1|0|\n",
    "|0|0|1|\n",
    "\n",
    "One-hot encoding results in a sparse vector representation (mostly zeros, hi dimensions). Word embeddings provide dense vector representations. Idea is to “embed” word into a lower-dimensionality space.\n",
    "The dimensions of this space are typically defined by word context, i.e., semantically similar words are embedded near each other.  \n",
    "\n",
    "Popular word-embedding algorithms:  \n",
    "- Point-wise mutual information (PMI)  \n",
    "- Word2Vec: Skip-gram or CBOW  \n",
    "- GLoVE - Global Vectors for Word Representation  \n",
    "\n",
    "*Word2Vec*  \n",
    "CBOW model predict missing word (focus word) using context (surrounding words). Skip gram model predicts context based on the word in focus. Context is a fixed number of words to the left and right of the word in focus. Maximize average log probability of context words co-occurring with focus words.\n",
    "\n",
    "![Word2Vec](word2vec.png)\n",
    "\n",
    "One way to understand the concept of embedding is to thing of words being embedded in a higher dimensional vector space.\n",
    "\n",
    "![Vector Space](vector_space.png)\n",
    "\n",
    "This comes from the deck of slides: http://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994\n",
    "\n",
    "For our lab, we will use Keras' `Embedding` layer to automatically build SkipGram embeddings for our text collection.\n",
    "\n",
    "The layer takes arguments that define the mapping including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value that will be seen as an integer). The layer also allows you to specify the dimensionality for each word vector, called the output dimension.\n",
    "\n",
    "We would like to use a word embedding representation for the IMDB dataset.\n",
    "\n",
    "If we are only interested in the 5,000 most frequently used words in the dataset, our vocabulary size will be 5,000. We can choose to use a 32-dimension vector to represent each word. Most word vectors are in the 100 to 200 dimension range for general NLP problems.\n",
    "\n",
    "Embeddings will help us match words by meaning, not just surface form, and help capture word context.\n",
    "\n",
    "The word embedding representation is a true innovation. We will demonstrate what would have been considered world class results in 2011 with a relatively simple neural network.\n",
    "\n",
    "The first layer of our model will be an word embedding layer created using the Embedding class as follows:\n",
    "\n",
    "`Embedding(5000, 32, input_length=500)`\n",
    "\n",
    "The output of this first layer would be a matrix with the size 32×500 for a given review training or test pattern in integer format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncation and padding input examples\n",
    "\n",
    "We can use the Keras utility `sequence.pad_sequences()` to truncate or pad the dataset to a length of 500 for each observation using the function. Neural network models expect input examples to be the same dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras\n",
    "The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the  Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows you to build arbitrary graphs of layers.\n",
    "\n",
    "Here is the Sequential model:   \n",
    "```\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "```\n",
    "\n",
    "Stacking layers by using .add():  \n",
    "```\n",
    "from keras.layers import Dense\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "```\n",
    "\n",
    "Once your model looks good, configure its learning process with .compile():  \n",
    "```\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "                            \n",
    "If you need to, you can further configure your optimizer.   \n",
    "```\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "```\n",
    "\n",
    "You can iterate on your training data in batches:\n",
    "```\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "```\n",
    "\n",
    "More here:   \n",
    "\n",
    "https://keras.io/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Simple Multi-Layer Perceptron Model \n",
    "\n",
    "We can start off by developing a simple multi-layer perceptron (MLP) model with a single hidden layer. MLP's are also called a fully-connected network (FCN). They're \"traditional\" neural networks.\n",
    "\n",
    "Let’s start off by importing the classes and functions required for this model and initializing the random number generator to a constant value to ensure we can easily reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit our vocabulary to the 5,000 most common terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit each review length to 500 words. Longer reviews will be truncated. Shorter reviews will use zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with an embedding size of 32.\n",
    "\n",
    "The top_words, max_words, and embedding_size are considered `hyperparameters` which require tuning by conducting several iterations of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the first embedding layer will be a 32×500 sized tensor.\n",
    "\n",
    "We will flatten the embedding layer's output to one dimension, then use one dense hidden layer of 250 units with a rectified linear unit (ReLU) activation function. \n",
    "\n",
    "ReLU is a common nonlinearity, and is defined by a simple formula:\n",
    "$$ReLU(z) = max(0, z)$$\n",
    "Here is its graphical representation (and next to another activation, sigmoid, for comparison):\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*XxxiA0jJvPrHEJHD4z893g.png)\n",
    "\n",
    "The output layer has one neuron and we will use a sigmoid activation function to output values of 0 and 1 as predictions. If we had to classify more than two values, we would use softmax.\n",
    "\n",
    "The model uses binary cross-entropy loss:\n",
    "\n",
    "$$loss = -[y_t * log_2(y_p) + (1 - y_t)*log_2(1-y_p)]$$\n",
    "\n",
    "and is optimized using the efficient gradient descent based ADAM optimization procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_size, input_length=input_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to printing out the model summary, there are other ways to visualize our architecture.\n",
    "\n",
    "Yaml, which stands for \"YAML ain't markup language.\" Seriously. That's what it stands for. YAML will let us read what we've built. It can also be used to automatically define a network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate a graphic. Note: Graphviz and pydot need to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**   \n",
    "\n",
    "Take a moment and try to understand the model you just built and the summary. Note the number of parameters in the model. This relates to the number of weights that have to be learned using back propagation. The larger the number the parameters, the more complex the model. The more complex the model, the longer the training time and a need for larger training sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can fit the model. We will hold out 10% of the training data for validation. Validation data is used to \"test\" the model during training to see if our loss decreases and our acccurcy decreases. The model can be re-trained before deployment with all available data.\n",
    "\n",
    "We've specified 3 epochs, or number of iterations across the training data.\n",
    "\n",
    "Batch size defines how many examples we show to the network before updating weights using back propagation.\n",
    "\n",
    "We will perform final evaluation using the held-out test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "epochs=3\n",
    "batch_size=128\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example fits the model and summarizes the estimated performance. We can see that a realtively simple model achieves a score of nearly ~86% which is in the neighborhood of the original paper, with very little effort. And good engineering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the accuracy and training loss using the history from training the model. Since we only used 3 training epochs, these graphs will not show much. 10's or 100's of epochs is commong in more complex models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot model loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "I’m sure we can do better. Take a few minutes and experiment with the model and see if you can improve its performance on test accuracy.\n",
    "\n",
    "You can try adusting any of the hyperparameters: epochs, batch_size, embedding_size, input_length, top_words, the number of nodes (currently 250) in the dense layer, adding another hidden layer, or adding dropout or regularization.\n",
    "\n",
    "```\n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "```\n",
    "\n",
    "https://keras.io/regularizers/\n",
    "\n",
    "https://keras.io/layers/core/\n",
    "\n",
    "https://chrisalbon.com/deep_learning/keras/adding_dropout/\n",
    "\n",
    "```\n",
    "model.add(layers.Dropout(0.2))\n",
    "```\n",
    "\n",
    "Goldstar to the best model. Report your results below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP, take a deep breath and wait for instruction before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Dimensional Convolutional Neural Network Model \n",
    "\n",
    "Convolutional neural networks were designed to honor the spatial structure in image data while being robust to the position and orientation of learned objects in the scene.\n",
    "\n",
    "This same principle can be used on sequences, such as the one-dimensional sequence of words in a movie review. The same properties that make the CNN model attractive for learning to recognize objects in images can help to learn structure in paragraphs of words, namely the techniques invariance to the specific position of features.\n",
    "\n",
    "Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively.\n",
    "\n",
    "Again, let’s import the classes and functions needed for this example and initialize our random number generator to a constant value so that we can easily reproduce results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load and prepare our IMDB dataset as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "input_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=input_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our convolutional neural network model. This time, after the Embedding input layer, we insert a `Conv1D` layer. This convolutional layer has `32` feature maps and reads embedded word representations with a filter size (kernel_size) equal to 3. I.e., 3 vector elements of the word embedding at a time.\n",
    "\n",
    "The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# Add additional convolution layer\n",
    "#model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(layers.Dropout(0.2))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also fit the network the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, we are first presented with a summary of the network structure. We can see our convolutional layer preserves the dimensionality of our Embedding input layer of 32-dimensional input with a maximum of 500 words. The pooling layer compresses this representation by halving it.\n",
    "\n",
    "Running the example offers a small but welcome improvement over the neural network model above with an accuracy of nearly 87.85%.\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "It's your turn. See if you can improve the performance of the network by adjusting the hyperparameters, adding additional convolution layers, and playing with dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP, take a deep breath and wait for instruction before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "\n",
    "Below is an example of training an LSTM network.\n",
    "\n",
    "Notes:  \n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.   \n",
    "- Some configurations won't converge.  \n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP - will come back to if we have time\n",
    "# Log to tensorboard\n",
    "# tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6tf1.3keras]",
   "language": "python",
   "name": "conda-env-py3.6tf1.3keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
